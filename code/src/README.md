# 設定値の説明

`code/src/sample-transformers.py`の`config_dict'の詳細です

## 1. "train_batch_size": 2

学習（training）時のバッチサイズを決めるパラメータです。

1 ステップあたりモデルに入力するサンプル数をここで指定します。
一般的にはバッチサイズを大きくすると安定して学習が進みやすい場合がありますが、GPU メモリを大量に消費します。
バッチサイズを小さくするとメモリ使用量は減りますが、勾配の推定がやや不安定になる場合もあります。

## 2. "valid_batch_size": 2

検証（validation）時のバッチサイズを決めるパラメータです。

検証時にも複数サンプルを同時に forward 処理させます。
学習時より大きくしてもよいことが多いですが、同じくメモリ消費が増える点に注意が必要です。

## 3. "weight_decay": 0.1

Weight Decay（ウェイト減衰、L2 正則化）の係数を表します。

AdamW オプティマイザでは、重みの更新時に weight_decay 分だけパラメータを減衰させます。
一般的に 0.01 〜 0.1 程度の範囲で試行することが多く、過学習を抑える効果が期待できます。

## 4. "shuffle_buffer": 1000

ストリーミングデータセットのシャッフル時に使用するバッファサイズです。

load_dataset(..., streaming=True).shuffle(buffer_size=shuffle_buffer) のように、ストリーミングモードのデータをバッファに取り込み、その範囲でランダムに順序を混ぜながら供給する仕組みです。
バッファサイズを大きくすると「よりグローバルにシャッフル」しやすくなりますが、メモリ使用量も増えます。

## 5. "learning_rate": 2e-4

学習率（Learning Rate） です。

大きいほど学習が速く進む可能性がありますが、勾配が暴れる場合もあります。
小さいほど安定しますが、収束に時間がかかったり、うまく局所解にハマる可能性もあります。

## 6. "lr_scheduler_type": "cosine"

学習率を徐々に変動させるスケジューラ（Scheduler）の種類を表します。

"cosine" は Cosine Annealing という手法で、学習率をコサイン曲線に従って徐々に下げていきます。
他にも "linear" や "polynomial" などがあり、それぞれ学習率の減衰曲線が異なります。

## 7. "num_warmup_steps": 750

ウォームアップステップ数です。

学習開始直後から急に大きな学習率を使うと不安定になりがちなので、最初の 750 ステップまでは学習率を段階的に上昇させるなどの工夫をします。
"lr_scheduler_type": "cosine" や "linear" と組み合わせて使用し、ウォームアップ後に本来の学習率スケジューラに切り替えるという流れが一般的です。

## 8. "gradient_accumulation_steps": 16

勾配の累積ステップ数を指定します。

例えばバッチサイズが大きく取れない環境（GPU メモリが少ないなど）のとき、バッチサイズを小さくしたまま「勾配を 16 ステップ分ためてから 1 回のパラメータ更新を行う」仕組みです。
これによって「実質的なバッチサイズ」を train_batch_size × gradient_accumulation_steps に近づけることができます。
一方で学習にかかる「1 パラメータ更新あたりの計算量」は増えるため、実行速度は遅くなることもあります。

## 9. "max_train_steps": 50000

学習の最大ステップ数です。

for step in range(1, max_train_steps+1): ... のように最大 50,000 ステップまで学習し、終了します。
1 ステップ＝ 1 バッチ（勾配累積を含めてパラメータが更新されるタイミング）と捉えられます。

## 10. "max_eval_steps": -1

検証（evaluation）の際の最大ステップ数です。

-1 の場合、制限なしという扱いになります（実際にはストリーミングデータを最後まで取る、もしくは別途 StopIteration が起きるまで読むなどの実装次第）。
もし学習データや検証データが非常に大きい場合、時間短縮のために max_eval_steps を何ステップかに制限しておくこともあります。

## 11. "seq_length": 512

入力シーケンスの最大長を指定します。

テキストをトークナイズした際に、1 サンプルあたり最大で 512 トークンまでをモデルへ入力するという設定です。
大きくすると長い文脈を一度に学習できるメリットがありますが、メモリ使用量や計算量が急増します。
実際に「1024 だとメモリ不足で落ちるので 512 に落とした」というケースはよくあります。

## 12. "seed": 1

乱数シードを固定するためのシード値です。

訓練データのシャッフルや重みの初期化などに使われます。
デターミニスティックな再現性が必要な場合に固定しておきます。

## 13. "save_checkpoint_steps": 50000

チェックポイントを保存するステップ間隔です。

例えばステップが 50,000 に到達したタイミング（※これだとちょうど最終ステップにもなりますが）や、指定した複数タイミングでモデルを save_pretrained(...) するなど、途中保存する頻度を決める。
gradient_accumulation_steps の単位とは別で、このステップ数はパラメータ更新の回数に近いです。
